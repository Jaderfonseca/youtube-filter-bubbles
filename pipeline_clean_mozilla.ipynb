{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaderfonseca/youtube-filter-bubbles/blob/main/pipeline_clean_mozilla.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef5a487b",
      "metadata": {
        "id": "ef5a487b"
      },
      "source": [
        "# Exploring Filter Bubbles in YouTube Recommendations\n",
        "**Mini‑project** · Portfolio-ready version\n",
        "\n",
        "This notebook reproduces the pipeline used to collect a small corpus of YouTube videos by seed queries, build a similarity graph via TF‑IDF, cluster content, and compute simple diversity metrics. It is intentionally compact and reproducible for reviewers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72ea0206",
      "metadata": {
        "id": "72ea0206"
      },
      "source": [
        "## How to run\n",
        "1. Open in Google Colab.\n",
        "2. Run the **Setup** cell to install packages and securely input your **YouTube Data API v3** key.\n",
        "3. Execute cells in order: **Data collection → Preprocessing → TF‑IDF & Similarity → Clustering → Metrics → Plots**.\n",
        "4. Outputs are written to `data/` and figures to `figures/`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "765002ab",
      "metadata": {
        "id": "765002ab"
      },
      "source": [
        "**Folder structure**\n",
        "\n",
        "- `data/raw/` – raw CSVs from collection\n",
        "- `data/clean/` – deduplicated & text‑processed CSVs\n",
        "- `data/processed/` – TF‑IDF, similarity, clustering, metrics\n",
        "- `figures/` – saved plots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e8a49b2",
      "metadata": {
        "id": "6e8a49b2"
      },
      "source": [
        "## Setup\n",
        "Installs minimal dependencies, imports libraries, prompts for the API key via `getpass` (not stored), and creates local folders. This cell must run first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90046c03",
      "metadata": {
        "id": "90046c03"
      },
      "outputs": [],
      "source": [
        "# --- Setup (safe) ---\n",
        "# 1) Install minimal dependencies (quiet)\n",
        "!pip install -q google-api-python-client pandas scikit-learn networkx matplotlib joblib\n",
        "\n",
        "# 2) Imports\n",
        "import os, json, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from getpass import getpass\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 3) Read API key securely (user pastes when prompted)\n",
        "API_KEY = getpass(\"Paste your YouTube API key (input is hidden): \").strip()\n",
        "assert API_KEY, \"Empty API_KEY\"\n",
        "\n",
        "# 4) Create local folders (Colab's ephemeral FS)\n",
        "os.makedirs(\"data/raw\", exist_ok=True)\n",
        "os.makedirs(\"data/clean\", exist_ok=True)\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "\n",
        "print(\"Environment ready. Proceed cell-by-cell from data collection to plots.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16316c10",
      "metadata": {
        "id": "16316c10"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "API_KEY = API_KEY  # reuse your existing key variable\n",
        "SEARCH_URL = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "\n",
        "def search_paged(query, max_items=120):\n",
        "    \"\"\"\n",
        "    Fetch up to `max_items` videos for a query using pageToken pagination.\n",
        "    Returns a DataFrame with video_id, title, channel_title.\n",
        "    \"\"\"\n",
        "    rows, token = [], None\n",
        "    while len(rows) < max_items:\n",
        "        params = {\n",
        "            \"part\": \"snippet\",\n",
        "            \"q\": query,\n",
        "            \"type\": \"video\",\n",
        "            \"maxResults\": 50,      # API max per page\n",
        "            \"key\": API_KEY,\n",
        "        }\n",
        "        if token:\n",
        "            params[\"pageToken\"] = token\n",
        "        r = requests.get(SEARCH_URL, params=params)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        for it in data.get(\"items\", []):\n",
        "            rows.append({\n",
        "                \"video_id\": it[\"id\"][\"videoId\"],\n",
        "                \"title\": it[\"snippet\"].get(\"title\",\"\"),\n",
        "                \"description\": it[\"snippet\"].get(\"description\",\"\"),\n",
        "                \"channel_title\": it[\"snippet\"].get(\"channelTitle\",\"\"),\n",
        "            })\n",
        "            if len(rows) >= max_items:\n",
        "                break\n",
        "        token = data.get(\"nextPageToken\")\n",
        "        if not token:\n",
        "            break\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f182e6",
      "metadata": {
        "id": "b7f182e6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def topk_neighbors(seed_row, k=5):\n",
        "    \"\"\"\n",
        "    Given a seed row (with title/description), return top-k similar pool items.\n",
        "    Excludes the seed itself if it exists in the pool.\n",
        "    \"\"\"\n",
        "    seed_text = (str(seed_row.get(\"title\",\"\")) + \" \" + str(seed_row.get(\"description\",\"\"))).strip()\n",
        "    x_seed = tfidf.transform([seed_text])\n",
        "    sims = cosine_similarity(x_seed, X_pool)[0]  # shape: (n_pool,)\n",
        "    # Exclude the seed itself if present\n",
        "    seed_id = seed_row[\"video_id\"]\n",
        "    if seed_id in idx_by_id:\n",
        "        sims[idx_by_id[seed_id]] = -1.0\n",
        "    top_idx = np.argsort(-sims)[:k]\n",
        "    out = df_pool.iloc[top_idx].copy()\n",
        "    out[\"similarity\"] = sims[top_idx]\n",
        "    out[\"source_video_id\"] = seed_id\n",
        "    return out[[\"source_video_id\",\"video_id\",\"title\",\"channel_title\",\"similarity\"]]\n",
        "\n",
        "# Build edges for first 5 seeds\n",
        "edges_list = []\n",
        "for _, seed in df_seed.head(5).iterrows():\n",
        "    edges_list.append(topk_neighbors(seed, k=5))\n",
        "df_edges_step1 = pd.concat(edges_list, ignore_index=True)\n",
        "\n",
        "print(\"Edges (seed -> neighbor):\", len(df_edges_step1))\n",
        "display(df_edges_step1.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7266b6ce",
      "metadata": {
        "id": "7266b6ce"
      },
      "outputs": [],
      "source": [
        "# Loop over 2 seeds -> pool -> TF-IDF neighbors -> save RAW CSVs\n",
        "\n",
        "import requests, pandas as pd, numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "SEARCH_URL = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "\n",
        "def search_paged(query, max_items=150):\n",
        "    rows, token = [], None\n",
        "    while len(rows) < max_items:\n",
        "        params = {\n",
        "            \"part\": \"snippet\", \"q\": query, \"type\": \"video\",\n",
        "            \"maxResults\": 50, \"key\": API_KEY\n",
        "        }\n",
        "        if token: params[\"pageToken\"] = token\n",
        "        r = requests.get(SEARCH_URL, params=params); r.raise_for_status()\n",
        "        data = r.json()\n",
        "        for it in data.get(\"items\", []):\n",
        "            rows.append({\n",
        "                \"seed_query\": query,\n",
        "                \"video_id\": it[\"id\"][\"videoId\"],\n",
        "                \"title\": it[\"snippet\"].get(\"title\",\"\"),\n",
        "                \"description\": it[\"snippet\"].get(\"description\",\"\"),\n",
        "                \"channel_title\": it[\"snippet\"].get(\"channelTitle\",\"\"),\n",
        "            })\n",
        "            if len(rows) >= max_items: break\n",
        "        token = data.get(\"nextPageToken\")\n",
        "        if not token: break\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def build_edges_tfidf(df_pool, seeds_df, k=5):\n",
        "    text = (df_pool[\"title\"].fillna(\"\") + \" \" + df_pool[\"description\"].fillna(\"\")).values\n",
        "    tfidf = TfidfVectorizer(lowercase=True, stop_words=\"english\", max_features=20000)\n",
        "    X = tfidf.fit_transform(text)\n",
        "    idx_by_id = {vid: i for i, vid in enumerate(df_pool[\"video_id\"].tolist())}\n",
        "\n",
        "    edges = []\n",
        "    for _, seed in seeds_df.iterrows():\n",
        "        seed_text = (str(seed.get(\"title\",\"\")) + \" \" + str(seed.get(\"description\",\"\"))).strip()\n",
        "        x_seed = tfidf.transform([seed_text])\n",
        "        sims = cosine_similarity(x_seed, X)[0]\n",
        "        if seed[\"video_id\"] in idx_by_id:\n",
        "            sims[idx_by_id[seed[\"video_id\"]]] = -1.0  # exclude itself\n",
        "        top_idx = np.argsort(-sims)[:k]\n",
        "        out = df_pool.iloc[top_idx].copy()\n",
        "        for i, row in out.iterrows():\n",
        "            edges.append({\n",
        "                \"seed_query\": seed[\"seed_query\"],\n",
        "                \"source_video_id\": seed[\"video_id\"],\n",
        "                \"video_id\": row[\"video_id\"],\n",
        "                \"title\": row[\"title\"],\n",
        "                \"channel_title\": row[\"channel_title\"],\n",
        "                \"similarity\": float(sims[i])\n",
        "            })\n",
        "    return pd.DataFrame(edges)\n",
        "\n",
        "SEEDS = [\"healthy cooking\", \"beginner guitar\"]\n",
        "\n",
        "all_nodes, all_edges = [], []\n",
        "\n",
        "for q in SEEDS:\n",
        "    df_pool = search_paged(q, max_items=150)\n",
        "    # 5 initial videos as \"seeds\" within the pool\n",
        "    df_seed_local = df_pool.head(5).copy()\n",
        "    all_nodes.append(df_pool)\n",
        "    df_edges = build_edges_tfidf(df_pool, df_seed_local, k=5)\n",
        "    all_edges.append(df_edges)\n",
        "    print(f\"[{q}] pool={len(df_pool)} edges={len(df_edges)}\")\n",
        "\n",
        "df_nodes_raw = pd.concat(all_nodes, ignore_index=True).drop_duplicates(subset=[\"video_id\"])\n",
        "df_edges_raw = pd.concat(all_edges, ignore_index=True)\n",
        "\n",
        "df_nodes_raw.to_csv(\"videos_raw.csv\", index=False)\n",
        "df_edges_raw.to_csv(\"edges_raw.csv\", index=False)\n",
        "print(\"Saved: videos_raw.csv and edges_raw.csv\")\n",
        "print(df_nodes_raw.head(2)); print(df_edges_raw.head(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02636b4a",
      "metadata": {
        "id": "02636b4a"
      },
      "outputs": [],
      "source": [
        "# === 3 seeds -> pool -> TF-IDF neighbors -> save RAW CSVs ===\n",
        "import os, requests, pandas as pd, numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "API_KEY = API_KEY  # reuse your key variable\n",
        "SEARCH_URL = \"https://www.googleapis.com/youtube/v3/search\"\n",
        "\n",
        "SEEDS = [\"healthy cooking\", \"beginner guitar\", \"stretching exercises\"]  # 3 neutral seeds\n",
        "POOL_SIZE = 150   # per seed\n",
        "TOPK = 5          # neighbors per seed video\n",
        "SEED_COUNT = 5    # seed videos per query\n",
        "\n",
        "# --- helpers ---\n",
        "def search_paged(query, max_items=150):\n",
        "    rows, token = [], None\n",
        "    while len(rows) < max_items:\n",
        "        params = {\n",
        "            \"part\": \"snippet\", \"q\": query, \"type\": \"video\",\n",
        "            \"maxResults\": 50, \"key\": API_KEY\n",
        "        }\n",
        "        if token: params[\"pageToken\"] = token\n",
        "        r = requests.get(SEARCH_URL, params=params); r.raise_for_status()\n",
        "        data = r.json()\n",
        "        for it in data.get(\"items\", []):\n",
        "            rows.append({\n",
        "                \"seed_query\": query,\n",
        "                \"video_id\": it[\"id\"][\"videoId\"],\n",
        "                \"title\": it[\"snippet\"].get(\"title\",\"\"),\n",
        "                \"description\": it[\"snippet\"].get(\"description\",\"\"),\n",
        "                \"channel_title\": it[\"snippet\"].get(\"channelTitle\",\"\"),\n",
        "            })\n",
        "            if len(rows) >= max_items: break\n",
        "        token = data.get(\"nextPageToken\")\n",
        "        if not token: break\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def build_edges_tfidf(df_pool, seeds_df, k=5):\n",
        "    text = (df_pool[\"title\"].fillna(\"\") + \" \" + df_pool[\"description\"].fillna(\"\")).values\n",
        "    tfidf = TfidfVectorizer(lowercase=True, stop_words=\"english\", max_features=20000)\n",
        "    X = tfidf.fit_transform(text)\n",
        "    idx_by_id = {vid: i for i, vid in enumerate(df_pool[\"video_id\"].tolist())}\n",
        "\n",
        "    edges = []\n",
        "    for _, seed in seeds_df.iterrows():\n",
        "        seed_text = (str(seed.get(\"title\",\"\")) + \" \" + str(seed.get(\"description\",\"\"))).strip()\n",
        "        x_seed = tfidf.transform([seed_text])\n",
        "        sims = cosine_similarity(x_seed, X)[0]\n",
        "        if seed[\"video_id\"] in idx_by_id:\n",
        "            sims[idx_by_id[seed[\"video_id\"]]] = -1.0\n",
        "        top_idx = np.argsort(-sims)[:k]\n",
        "        out = df_pool.iloc[top_idx].copy()\n",
        "        for i in top_idx:\n",
        "            row = df_pool.iloc[i]\n",
        "            edges.append({\n",
        "                \"seed_query\": seed[\"seed_query\"],\n",
        "                \"source_video_id\": seed[\"video_id\"],\n",
        "                \"video_id\": row[\"video_id\"],\n",
        "                \"title\": row[\"title\"],\n",
        "                \"channel_title\": row[\"channel_title\"],\n",
        "                \"similarity\": float(sims[i])\n",
        "            })\n",
        "    return pd.DataFrame(edges)\n",
        "\n",
        "# --- run for 3 seeds ---\n",
        "all_nodes, all_edges = [], []\n",
        "for q in SEEDS:\n",
        "    df_pool = search_paged(q, max_items=POOL_SIZE)\n",
        "    df_seed_local = df_pool.head(SEED_COUNT).copy()\n",
        "    all_nodes.append(df_pool)\n",
        "    df_edges = build_edges_tfidf(df_pool, df_seed_local, k=TOPK)\n",
        "    all_edges.append(df_edges)\n",
        "    print(f\"[{q}] pool={len(df_pool)} edges={len(df_edges)}\")\n",
        "\n",
        "df_nodes_raw = pd.concat(all_nodes, ignore_index=True).drop_duplicates(subset=[\"video_id\"])\n",
        "df_edges_raw = pd.concat(all_edges, ignore_index=True)\n",
        "\n",
        "# --- save under data/raw/ ---\n",
        "os.makedirs(\"data/raw\", exist_ok=True)\n",
        "nodes_path = \"data/raw/videos_raw.csv\"\n",
        "edges_path = \"data/raw/edges_raw.csv\"\n",
        "\n",
        "df_nodes_raw.to_csv(nodes_path, index=False)\n",
        "df_edges_raw.to_csv(edges_path, index=False)\n",
        "\n",
        "print(f\"Saved:\\n  {nodes_path} ({len(df_nodes_raw)} rows)\\n  {edges_path} ({len(df_edges_raw)} rows)\")\n",
        "df_nodes_raw.head(), df_edges_raw.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c78622",
      "metadata": {
        "id": "52c78622"
      },
      "outputs": [],
      "source": [
        "!pip install pandas scikit-learn requests -q\n",
        "\n",
        "import os, requests, pandas as pd, numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from getpass import getpass\n",
        "API_KEY = getpass(\"Paste your Youtube API key(input is hidden):\").strip()\n",
        "\n",
        "# 2) Where we’ll save raw files\n",
        "os.makedirs(\"data/raw\", exist_ok=True)\n",
        "\n",
        "SEARCH_URL = \"https://www.googleapis.com/youtube/v3/search\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0674a6d9",
      "metadata": {
        "id": "0674a6d9"
      },
      "outputs": [],
      "source": [
        "# Load RAW, clean, save CLEAN\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load raw CSVs\n",
        "nodes_raw_path = \"data/raw/videos_raw.csv\"\n",
        "edges_raw_path = \"data/raw/edges_raw.csv\"\n",
        "\n",
        "df_videos = pd.read_csv(nodes_raw_path)\n",
        "df_edges  = pd.read_csv(edges_raw_path)\n",
        "\n",
        "print(\"Before cleaning:\")\n",
        "print(\" - videos rows:\", len(df_videos))\n",
        "print(\" - edges  rows:\", len(df_edges))\n",
        "\n",
        "# 2) Basic cleaning\n",
        "# 2a) drop duplicate videos by ID\n",
        "df_videos = df_videos.drop_duplicates(subset=[\"video_id\"]).copy()\n",
        "\n",
        "# 2b) drop fully-empty columns (if any)\n",
        "df_videos = df_videos.dropna(axis=1, how=\"all\")\n",
        "df_edges  = df_edges.dropna(axis=1, how=\"all\")\n",
        "\n",
        "# 2c) drop duplicate edges (exact duplicates)\n",
        "df_edges = df_edges.drop_duplicates().copy()\n",
        "\n",
        "# 2d) (optional) keep only edges whose nodes exist in videos\n",
        "if \"video_id\" in df_edges.columns and \"source_video_id\" in df_edges.columns:\n",
        "    valid_ids = set(df_videos[\"video_id\"])\n",
        "    df_edges = df_edges[\n",
        "        df_edges[\"video_id\"].isin(valid_ids) & df_edges[\"source_video_id\"].isin(valid_ids)\n",
        "    ].copy()\n",
        "\n",
        "print(\"\\nAfter cleaning:\")\n",
        "print(\" - videos rows:\", len(df_videos))\n",
        "print(\" - edges  rows:\", len(df_edges))\n",
        "\n",
        "# 3) Save CLEAN files\n",
        "os.makedirs(\"data/clean\", exist_ok=True)\n",
        "nodes_clean_path = \"data/clean/videos_clean.csv\"\n",
        "edges_clean_path = \"data/clean/edges_clean.csv\"\n",
        "\n",
        "df_videos.to_csv(nodes_clean_path, index=False)\n",
        "df_edges.to_csv(edges_clean_path, index=False)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\" -\", nodes_clean_path)\n",
        "print(\" -\", edges_clean_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3b48cd",
      "metadata": {
        "id": "fb3b48cd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CLEAN files\n",
        "df_videos = pd.read_csv(\"data/clean/videos_clean.csv\")\n",
        "df_edges  = pd.read_csv(\"data/clean/edges_clean.csv\")\n",
        "\n",
        "# ---- sanity checks ----\n",
        "print(\"Videos (rows):\", len(df_videos))\n",
        "print(\"Edges  (rows):\", len(df_edges))\n",
        "\n",
        "# Preview\n",
        "display(df_videos.head())\n",
        "display(df_edges.head())\n",
        "\n",
        "# Counts by seed\n",
        "print(\"\\nVideos per seed_query:\")\n",
        "print(df_videos[\"seed_query\"].value_counts())\n",
        "\n",
        "# Basic graph sanity: how many unique source nodes in edges?\n",
        "if {\"source_video_id\",\"video_id\"}.issubset(df_edges.columns):\n",
        "    print(\"\\nUnique source nodes in edges:\", df_edges[\"source_video_id\"].nunique())\n",
        "    print(\"Unique target nodes in edges:\", df_edges[\"video_id\"].nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb31dcb5",
      "metadata": {
        "id": "bb31dcb5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load cleaned data from Night 6\n",
        "videos_clean_path = \"data/clean/videos_clean.csv\"\n",
        "edges_clean_path  = \"data/clean/edges_clean.csv\"\n",
        "\n",
        "assert os.path.exists(videos_clean_path), \"Missing data/clean/videos_clean.csv — rerun Night 6\"\n",
        "assert os.path.exists(edges_clean_path),  \"Missing data/clean/edges_clean.csv — rerun Night 6\"\n",
        "\n",
        "df_videos = pd.read_csv(videos_clean_path)\n",
        "df_edges  = pd.read_csv(edges_clean_path)\n",
        "\n",
        "print(\"Loaded:\", len(df_videos), \"videos;\", len(df_edges), \"edges\")\n",
        "df_videos.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61df7cb",
      "metadata": {
        "id": "c61df7cb"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "out_path = \"data/processed/videos_text.csv\"\n",
        "df_videos.to_csv(out_path, index=False)\n",
        "print(\"Saved:\", out_path, \"with\", len(df_videos), \"rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa2d149",
      "metadata": {
        "id": "efa2d149"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "videos_path = \"/content/drive/MyDrive/videos_text.csv\"\n",
        "edges_path  = \"/content/drive/MyDrive/edges_clean.csv\"\n",
        "\n",
        "df_videos = pd.read_csv(videos_path)\n",
        "df_edges  = pd.read_csv(edges_path)\n",
        "\n",
        "print(\"Videos:\", len(df_videos))\n",
        "print(\"Edges:\", len(df_edges))\n",
        "df_videos.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "467b34af",
      "metadata": {
        "id": "467b34af"
      },
      "outputs": [],
      "source": [
        "!pip install networkx matplotlib -q\n",
        "\n",
        "import os, pandas as pd\n",
        "\n",
        "VIDEOS_PATH = \"data/clean/videos_clean.csv\"   # ou \"/content/drive/MyDrive/videos_clean.csv\"\n",
        "EDGES_PATH  = \"data/clean/edges_clean.csv\"    # ou \"/content/drive/MyDrive/edges_clean.csv\"\n",
        "\n",
        "df_videos = pd.read_csv(VIDEOS_PATH)\n",
        "df_edges  = pd.read_csv(EDGES_PATH)\n",
        "\n",
        "print(\"Loaded:\", len(df_videos), \"videos;\", len(df_edges), \"edges\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05c11b3",
      "metadata": {
        "id": "a05c11b3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "VIDEOS_PATH = \"/content/drive/MyDrive/videos_clean.csv\"\n",
        "EDGES_PATH  = \"/content/drive/MyDrive/edges_clean.csv\"\n",
        "\n",
        "df_videos = pd.read_csv(VIDEOS_PATH)\n",
        "df_edges  = pd.read_csv(EDGES_PATH)\n",
        "\n",
        "print(\"Loaded:\", len(df_videos), \"videos;\", len(df_edges), \"edges\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2841a4",
      "metadata": {
        "id": "ce2841a4"
      },
      "outputs": [],
      "source": [
        "!pip install networkx matplotlib -q\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "VIDEOS_PATH = \"data/clean/videos_clean.csv\"\n",
        "EDGES_PATH  = \"data/clean/edges_clean.csv\"\n",
        "\n",
        "df_videos = pd.read_csv(VIDEOS_PATH)\n",
        "df_edges  = pd.read_csv(EDGES_PATH)\n",
        "\n",
        "print(\"Loaded:\", len(df_videos), \"videos;\", len(df_edges), \"edges\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca76ce53",
      "metadata": {
        "id": "ca76ce53"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# build undirected graph from edges\n",
        "G = nx.from_pandas_edgelist(\n",
        "    df_edges,\n",
        "    source=\"source_video_id\",\n",
        "    target=\"video_id\",\n",
        "    edge_attr=[\"similarity\"],\n",
        "    create_using=nx.Graph()\n",
        ")\n",
        "\n",
        "# attach seed attribute to nodes\n",
        "for n in G.nodes():\n",
        "    G.nodes[n][\"seed\"] = seed_per_video.get(n, \"unknown\")\n",
        "\n",
        "# node size by degree\n",
        "deg = dict(G.degree())\n",
        "node_sizes = [max(50, deg[n]*60) for n in G.nodes()]\n",
        "\n",
        "# color by seed\n",
        "seeds = sorted({G.nodes[n][\"seed\"] for n in G.nodes()})\n",
        "seed_to_int = {s:i for i,s in enumerate(seeds)}\n",
        "node_colors = [seed_to_int[G.nodes[n][\"seed\"]] for n in G.nodes()]\n",
        "\n",
        "# layout\n",
        "pos = nx.spring_layout(G, seed=42, k=0.35)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.8)\n",
        "nx.draw_networkx_nodes(\n",
        "    G, pos,\n",
        "    node_color=node_colors,\n",
        "    node_size=node_sizes,\n",
        "    cmap=plt.cm.tab10,\n",
        "    alpha=0.9\n",
        ")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"YouTube mini-graph: video similarity by seed\", fontsize=12)\n",
        "\n",
        "# simple legend\n",
        "import matplotlib.patches as mpatches\n",
        "patches = [mpatches.Patch(color=plt.cm.tab10(seed_to_int[s]/max(1,len(seeds)-1)), label=s) for s in seeds]\n",
        "plt.legend(handles=patches, title=\"Seed\", loc=\"lower left\", bbox_to_anchor=(1.02, 0.02))\n",
        "\n",
        "# save\n",
        "import os\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/graph_overview.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved figure to: figures/graph_overview.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8761fba",
      "metadata": {
        "id": "f8761fba"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to processed file saved in Night 7\n",
        "PROCESSED_PATH = \"/content/drive/MyDrive/videos_text.csv\"\n",
        "\n",
        "df_videos = pd.read_csv(PROCESSED_PATH)\n",
        "\n",
        "print(\"Loaded:\", len(df_videos), \"rows\")\n",
        "print(\"Columns:\", df_videos.columns.tolist())\n",
        "df_videos.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "571b74ee",
      "metadata": {
        "id": "571b74ee"
      },
      "outputs": [],
      "source": [
        "# Night 10 — TF-IDF + similarity and save artifacts\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "\n",
        "# Assumes df_videos is already loaded and has a 'text' column (from Night 7)\n",
        "assert \"text\" in df_videos.columns, \"Missing 'text' column. Load videos_text.csv or rebuild Night 7.\"\n",
        "\n",
        "# 1) TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = vectorizer.fit_transform(df_videos[\"text\"])\n",
        "print(\"TF-IDF shape:\", tfidf_matrix.shape)\n",
        "\n",
        "# Save TF-IDF artifacts\n",
        "np.save(\"data/processed/tfidf_shape.npy\", np.array(tfidf_matrix.shape))\n",
        "with open(\"data/processed/tfidf_vocabulary.json\", \"w\") as f:\n",
        "    json.dump(vectorizer.vocabulary_, f)\n",
        "# (Optional) store the sparse matrix as .npy (dense is small here) or skip if large datasets\n",
        "S = cosine_similarity(tfidf_matrix)   # (N x N) dense; OK for ~435 videos\n",
        "np.save(\"data/processed/similarity_matrix.npy\", S)\n",
        "\n",
        "# 2) Top-K similar pairs as an edges CSV (compact and handy)\n",
        "TOPK = 5\n",
        "rows = []\n",
        "np.fill_diagonal(S, 0.0)\n",
        "for i in range(S.shape[0]):\n",
        "    idx = np.argpartition(-S[i], TOPK)[:TOPK]\n",
        "    idx = idx[np.argsort(-S[i, idx])]\n",
        "    for j in idx:\n",
        "        rows.append({\n",
        "            \"src_idx\": i,\n",
        "            \"src_video_id\": df_videos.iloc[i][\"video_id\"],\n",
        "            \"dst_idx\": int(j),\n",
        "            \"dst_video_id\": df_videos.iloc[j][\"video_id\"],\n",
        "            \"similarity\": float(S[i, j])\n",
        "        })\n",
        "\n",
        "df_topk = pd.DataFrame(rows)\n",
        "df_topk.to_csv(\"data/processed/similarity_topk_edges.csv\", index=False)\n",
        "\n",
        "# 3) (Optional) save tokens if you created them in Night 9\n",
        "if \"tokens\" in df_videos.columns:\n",
        "    df_videos.to_csv(\"data/processed/videos_tokens.csv\", index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" - data/processed/tfidf_shape.npy\")\n",
        "print(\" - data/processed/tfidf_vocabulary.json\")\n",
        "print(\" - data/processed/similarity_matrix.npy\")\n",
        "print(\" - data/processed/similarity_topk_edges.csv\")\n",
        "if \"tokens\" in df_videos.columns:\n",
        "    print(\" - data/processed/videos_tokens.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3cf5092",
      "metadata": {
        "id": "b3cf5092"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "def top_terms(cluster_id, topn=12):\n",
        "    # centroid weights for this cluster\n",
        "    centroid = kmeans.cluster_centers_[cluster_id]\n",
        "    idx = np.argsort(-centroid)[:topn]\n",
        "    return feature_names[idx].tolist()\n",
        "\n",
        "for c in range(N_CLUSTERS):\n",
        "    print(f\"Cluster {c}: \", top_terms(c, topn=12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19064129",
      "metadata": {
        "id": "19064129"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "df_videos.to_csv(\"data/processed/videos_with_clusters.csv\", index=False)\n",
        "\n",
        "# tiny summary CSV\n",
        "cluster_summary = (\n",
        "    df_videos.groupby([\"cluster\",\"seed_query\"])\n",
        "             .size()\n",
        "             .reset_index(name=\"count\")\n",
        ")\n",
        "cluster_summary.to_csv(\"data/processed/cluster_seed_counts.csv\", index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" - data/processed/videos_with_clusters.csv\")\n",
        "print(\" - data/processed/cluster_seed_counts.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3606a85b",
      "metadata": {
        "id": "3606a85b"
      },
      "outputs": [],
      "source": [
        "# NIGHT 10–11 CHECKLIST (single cell)\n",
        "import os, json, numpy as np, pandas as pd\n",
        "\n",
        "# Adjust if you us e another Drive folder name\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/yt_mini_project\"\n",
        "\n",
        "def exists(p):\n",
        "    return os.path.exists(p)\n",
        "\n",
        "def ok(name, cond):\n",
        "    print(f\"[{'OK' if cond else 'MISS'}] {name}\")\n",
        "\n",
        "# 1) Local files expected\n",
        "local_expected = [\n",
        "    \"data/clean/videos_clean.csv\",\n",
        "    \"data/clean/edges_clean.csv\",\n",
        "    \"data/processed/similarity_matrix.npy\",\n",
        "    \"data/processed/similarity_topk_edges.csv\",\n",
        "    \"data/processed/tfidf_vectorizer.joblib\"  # if you saved via Option B\n",
        "]\n",
        "print(\"== Local files ==\")\n",
        "for p in local_expected:\n",
        "    ok(p, exists(p))\n",
        "\n",
        "# 2) Drive files expected\n",
        "drive_expected = [\n",
        "    f\"{DRIVE_BASE}/data/clean/videos_clean.csv\",\n",
        "    f\"{DRIVE_BASE}/data/clean/edges_clean.csv\",\n",
        "    f\"{DRIVE_BASE}/data/processed/similarity_matrix.npy\",\n",
        "    f\"{DRIVE_BASE}/data/processed/similarity_topk_edges.csv\",\n",
        "    f\"{DRIVE_BASE}/data/processed/tfidf_vectorizer.joblib\",\n",
        "    f\"{DRIVE_BASE}/figures/graph_overview.png\",\n",
        "    f\"{DRIVE_BASE}/figures/graph_lcc.png\",\n",
        "]\n",
        "print(\"\\n== Drive files ==\")\n",
        "for p in drive_expected:\n",
        "    ok(p, exists(p))\n",
        "\n",
        "# 3) Load dataframes and check columns\n",
        "print(\"\\n== DataFrame sanity ==\")\n",
        "try:\n",
        "    df_videos = pd.read_csv(\"data/clean/videos_clean.csv\")\n",
        "    ok(\"Loaded data/clean/videos_clean.csv\", True)\n",
        "    ok(\"Column 'text' present\", \"text\" in df_videos.columns)\n",
        "    ok(\"Column 'seed_query' present\", \"seed_query\" in df_videos.columns)\n",
        "except Exception as e:\n",
        "    ok(\"Loaded data/clean/videos_clean.csv\", False)\n",
        "    print(\"Error:\", e)\n",
        "    df_videos = None\n",
        "\n",
        "try:\n",
        "    df_edges = pd.read_csv(\"data/clean/edges_clean.csv\")\n",
        "    ok(\"Loaded data/clean/edges_clean.csv\", True)\n",
        "    ok(\"Edges columns present\", {\"source_video_id\",\"video_id\"}.issubset(df_edges.columns))\n",
        "except Exception as e:\n",
        "    ok(\"Loaded data/clean/edges_clean.csv\", False)\n",
        "    print(\"Error:\", e)\n",
        "    df_edges = None\n",
        "\n",
        "# 4) Basic shapes and integrity\n",
        "if df_videos is not None:\n",
        "    print(\"\\nVideos rows:\", len(df_videos))\n",
        "    if \"cluster\" in df_videos.columns:\n",
        "        print(\"Cluster labels found. Counts:\")\n",
        "        print(df_videos[\"cluster\"].value_counts().sort_index())\n",
        "    else:\n",
        "        print(\"Column 'cluster' not found yet (run Night 11 KMeans).\")\n",
        "\n",
        "if df_edges is not None:\n",
        "    print(\"\\nEdges rows:\", len(df_edges))\n",
        "    if df_videos is not None and {\"source_video_id\",\"video_id\"}.issubset(df_edges.columns):\n",
        "        vids = set(df_videos[\"video_id\"])\n",
        "        bad = df_edges[~df_edges[\"source_video_id\"].isin(vids) | ~df_edges[\"video_id\"].isin(vids)]\n",
        "        print(\"Edges referencing missing nodes:\", len(bad))\n",
        "\n",
        "# 5) Similarity artifacts\n",
        "print(\"\\n== Similarity artifacts ==\")\n",
        "try:\n",
        "    S = np.load(\"data/processed/similarity_matrix.npy\")\n",
        "    print(\"similarity_matrix.npy shape:\", S.shape)\n",
        "    ok(\"Square matrix\", S.shape[0] == S.shape[1])\n",
        "except Exception as e:\n",
        "    print(\"Could not load similarity_matrix.npy:\", e)\n",
        "\n",
        "if os.path.exists(\"data/processed/similarity_topk_edges.csv\"):\n",
        "    try:\n",
        "        df_topk = pd.read_csv(\"data/processed/similarity_topk_edges.csv\")\n",
        "        print(\"similarity_topk_edges.csv rows:\", len(df_topk))\n",
        "        ok(\"TopK required columns\", {\"src_video_id\",\"dst_video_id\",\"similarity\"}.issubset(df_topk.columns))\n",
        "    except Exception as e:\n",
        "        print(\"Could not read similarity_topk_edges.csv:\", e)\n",
        "\n",
        "# 6) Figures presence\n",
        "print(\"\\n== Figures ==\")\n",
        "ok(\"figures/graph_overview.png\", os.path.exists(\"figures/graph_overview.png\"))\n",
        "ok(\"figures/graph_lcc.png\", os.path.exists(\"figures/graph_lcc.png\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e34c331",
      "metadata": {
        "id": "6e34c331"
      },
      "outputs": [],
      "source": [
        "# Build + save similarity_topk_edges.csv and back up to Drive\n",
        "\n",
        "import numpy as np, pandas as pd, os\n",
        "\n",
        "# 1) Load the processed videos WITH 'text' (Night 7)\n",
        "df_videos_order = pd.read_csv(\"/content/drive/MyDrive/videos_text.csv\")  # has column 'text'\n",
        "S = np.load(\"data/processed/similarity_matrix.npy\")\n",
        "\n",
        "assert len(df_videos_order) == S.shape[0], \"Row count mismatch. Recompute TF-IDF/Similarity with this df.\"\n",
        "\n",
        "# 2) Create Top-K edges for ALL videos\n",
        "TOPK = 5\n",
        "rows = []\n",
        "np.fill_diagonal(S, 0.0)\n",
        "for i in range(S.shape[0]):\n",
        "    idx = np.argpartition(-S[i], TOPK)[:TOPK]\n",
        "    idx = idx[np.argsort(-S[i, idx])]\n",
        "    for j in idx:\n",
        "        rows.append({\n",
        "            \"src_idx\": i,\n",
        "            \"src_video_id\": df_videos_order.iloc[i][\"video_id\"],\n",
        "            \"dst_idx\": int(j),\n",
        "            \"dst_video_id\": df_videos_order.iloc[j][\"video_id\"],\n",
        "            \"similarity\": float(S[i, j]),\n",
        "        })\n",
        "\n",
        "df_topk = pd.DataFrame(rows)\n",
        "\n",
        "# 3) Save locally\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "out_local = \"data/processed/similarity_topk_edges.csv\"\n",
        "df_topk.to_csv(out_local, index=False)\n",
        "print(\"Saved:\", out_local, \"rows:\", len(df_topk))\n",
        "\n",
        "# 4) Backup to Drive\n",
        "BASE = \"/content/drive/MyDrive/yt_mini_project\"\n",
        "os.makedirs(f\"{BASE}/data/processed\", exist_ok=True)\n",
        "out_drive = f\"{BASE}/data/processed/similarity_topk_edges.csv\"\n",
        "df_topk.to_csv(out_drive, index=False)\n",
        "print(\"Backed up to:\", out_drive)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29b9003",
      "metadata": {
        "id": "a29b9003"
      },
      "outputs": [],
      "source": [
        "# NIGHT 10–11 CHECKLIST (single cell)\n",
        "import os, json, numpy as np, pandas as pd\n",
        "\n",
        "# Adjust if you us e another Drive folder name\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/yt_mini_project\"\n",
        "\n",
        "def exists(p):\n",
        "    return os.path.exists(p)\n",
        "\n",
        "def ok(name, cond):\n",
        "    print(f\"[{'OK' if cond else 'MISS'}] {name}\")\n",
        "\n",
        "# 1) Local files expected\n",
        "local_expected = [\n",
        "    \"data/clean/videos_clean.csv\",\n",
        "    \"data/clean/edges_clean.csv\",\n",
        "    \"data/processed/similarity_matrix.npy\",\n",
        "    \"data/processed/similarity_topk_edges.csv\",\n",
        "    \"data/processed/tfidf_vectorizer.joblib\"  # if you saved via Option B\n",
        "]\n",
        "print(\"== Local files ==\")\n",
        "for p in local_expected:\n",
        "    ok(p, exists(p))\n",
        "\n",
        "# 2) Drive files expected\n",
        "drive_expected = [\n",
        "    f\"{DRIVE_BASE}/data/clean/videos_clean.csv\",\n",
        "    f\"{DRIVE_BASE}/data/clean/edges_clean.csv\",\n",
        "    f\"{DRIVE_BASE}/data/processed/similarity_matrix.npy\",\n",
        "    f\"{DRIVE_BASE}/data/processed/similarity_topk_edges.csv\",\n",
        "    f\"{DRIVE_BASE}/data/processed/tfidf_vectorizer.joblib\",\n",
        "    f\"{DRIVE_BASE}/figures/graph_overview.png\",\n",
        "    f\"{DRIVE_BASE}/figures/graph_lcc.png\",\n",
        "]\n",
        "print(\"\\n== Drive files ==\")\n",
        "for p in drive_expected:\n",
        "    ok(p, exists(p))\n",
        "\n",
        "# 3) Load dataframes and check columns\n",
        "print(\"\\n== DataFrame sanity ==\")\n",
        "try:\n",
        "    df_videos = pd.read_csv(\"data/clean/videos_clean.csv\")\n",
        "    ok(\"Loaded data/clean/videos_clean.csv\", True)\n",
        "    ok(\"Column 'text' present\", \"text\" in df_videos.columns)\n",
        "    ok(\"Column 'seed_query' present\", \"seed_query\" in df_videos.columns)\n",
        "except Exception as e:\n",
        "    ok(\"Loaded data/clean/videos_clean.csv\", False)\n",
        "    print(\"Error:\", e)\n",
        "    df_videos = None\n",
        "\n",
        "try:\n",
        "    df_edges = pd.read_csv(\"data/clean/edges_clean.csv\")\n",
        "    ok(\"Loaded data/clean/edges_clean.csv\", True)\n",
        "    ok(\"Edges columns present\", {\"source_video_id\",\"video_id\"}.issubset(df_edges.columns))\n",
        "except Exception as e:\n",
        "    ok(\"Loaded data/clean/edges_clean.csv\", False)\n",
        "    print(\"Error:\", e)\n",
        "    df_edges = None\n",
        "\n",
        "# 4) Basic shapes and integrity\n",
        "if df_videos is not None:\n",
        "    print(\"\\nVideos rows:\", len(df_videos))\n",
        "    if \"cluster\" in df_videos.columns:\n",
        "        print(\"Cluster labels found. Counts:\")\n",
        "        print(df_videos[\"cluster\"].value_counts().sort_index())\n",
        "    else:\n",
        "        print(\"Column 'cluster' not found yet (run Night 11 KMeans).\")\n",
        "\n",
        "if df_edges is not None:\n",
        "    print(\"\\nEdges rows:\", len(df_edges))\n",
        "    if df_videos is not None and {\"source_video_id\",\"video_id\"}.issubset(df_edges.columns):\n",
        "        vids = set(df_videos[\"video_id\"])\n",
        "        bad = df_edges[~df_edges[\"source_video_id\"].isin(vids) | ~df_edges[\"video_id\"].isin(vids)]\n",
        "        print(\"Edges referencing missing nodes:\", len(bad))\n",
        "\n",
        "# 5) Similarity artifacts\n",
        "print(\"\\n== Similarity artifacts ==\")\n",
        "try:\n",
        "    S = np.load(\"data/processed/similarity_matrix.npy\")\n",
        "    print(\"similarity_matrix.npy shape:\", S.shape)\n",
        "    ok(\"Square matrix\", S.shape[0] == S.shape[1])\n",
        "except Exception as e:\n",
        "    print(\"Could not load similarity_matrix.npy:\", e)\n",
        "\n",
        "if os.path.exists(\"data/processed/similarity_topk_edges.csv\"):\n",
        "    try:\n",
        "        df_topk = pd.read_csv(\"data/processed/similarity_topk_edges.csv\")\n",
        "        print(\"similarity_topk_edges.csv rows:\", len(df_topk))\n",
        "        ok(\"TopK required columns\", {\"src_video_id\",\"dst_video_id\",\"similarity\"}.issubset(df_topk.columns))\n",
        "    except Exception as e:\n",
        "        print(\"Could not read similarity_topk_edges.csv:\", e)\n",
        "\n",
        "# 6) Figures presence\n",
        "print(\"\\n== Figures ==\")\n",
        "ok(\"figures/graph_overview.png\", os.path.exists(\"figures/graph_overview.png\"))\n",
        "ok(\"figures/graph_lcc.png\", os.path.exists(\"figures/graph_lcc.png\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636617e6",
      "metadata": {
        "id": "636617e6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_videos_text = pd.read_csv(\"/content/drive/MyDrive/videos_text.csv\")\n",
        "print(\"Columns:\", df_videos_text.columns)\n",
        "print(df_videos_text.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ff1956",
      "metadata": {
        "id": "95ff1956"
      },
      "outputs": [],
      "source": [
        "# NIGHT 10–11 CHECKLIST (revised to use videos_text.csv)\n",
        "import os, json, numpy as np, pandas as pd\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/yt_mini_project\"\n",
        "\n",
        "def exists(p):\n",
        "    return os.path.exists(p)\n",
        "\n",
        "def ok(name, cond):\n",
        "    print(f\"[{'OK' if cond else 'MISS'}] {name}\")\n",
        "\n",
        "# 1) Local files expected\n",
        "local_expected = [\n",
        "    \"data/clean/edges_clean.csv\",\n",
        "    \"data/processed/similarity_matrix.npy\",\n",
        "    \"data/processed/similarity_topk_edges.csv\",\n",
        "    \"data/processed/tfidf_vectorizer.joblib\"  # if you saved via Option B\n",
        "]\n",
        "print(\"== Local files ==\")\n",
        "for p in local_expected:\n",
        "    ok(p, exists(p))\n",
        "\n",
        "# 2) Drive files expected\n",
        "drive_expected = [\n",
        "    f\"{DRIVE_BASE}/data/clean/edges_clean.csv\",\n",
        "    f\"{DRIVE_BASE}/data/processed/similarity_matrix.npy\",\n",
        "    f\"{DRIVE_BASE}/data/processed/similarity_topk_edges.csv\",\n",
        "    f\"{DRIVE_BASE}/data/processed/tfidf_vectorizer.joblib\",\n",
        "    f\"{DRIVE_BASE}/figures/graph_overview.png\",\n",
        "    f\"{DRIVE_BASE}/figures/graph_lcc.png\",\n",
        "    f\"{DRIVE_BASE}/videos_text.csv\"\n",
        "]\n",
        "print(\"\\n== Drive files ==\")\n",
        "for p in drive_expected:\n",
        "    ok(p, exists(p))\n",
        "\n",
        "# 3) Load videos_text (with text column)\n",
        "print(\"\\n== DataFrame sanity (videos_text.csv) ==\")\n",
        "try:\n",
        "    df_videos = pd.read_csv(f\"{DRIVE_BASE}/videos_text.csv\")\n",
        "    ok(\"Loaded videos_text.csv\", True)\n",
        "    ok(\"Column 'text' present\", \"text\" in df_videos.columns)\n",
        "    ok(\"Column 'seed_query' present\", \"seed_query\" in df_videos.columns)\n",
        "except Exception as e:\n",
        "    ok(\"Loaded videos_text.csv\", False)\n",
        "    print(\"Error:\", e)\n",
        "    df_videos = None\n",
        "\n",
        "# 4) Load edges_clean\n",
        "print(\"\\n== Edges check ==\")\n",
        "try:\n",
        "    df_edges = pd.read_csv(\"data/clean/edges_clean.csv\")\n",
        "    ok(\"Loaded data/clean/edges_clean.csv\", True)\n",
        "    ok(\"Edges columns present\", {\"source_video_id\",\"video_id\"}.issubset(df_edges.columns))\n",
        "except Exception as e:\n",
        "    ok(\"Loaded data/clean/edges_clean.csv\", False)\n",
        "    print(\"Error:\", e)\n",
        "    df_edges = None\n",
        "\n",
        "# 5) Shapes + clusters\n",
        "if df_videos is not None:\n",
        "    print(\"\\nVideos rows:\", len(df_videos))\n",
        "    if \"cluster\" in df_videos.columns:\n",
        "        print(\"Cluster labels found. Counts:\")\n",
        "        print(df_videos[\"cluster\"].value_counts().sort_index())\n",
        "    else:\n",
        "        print(\"Column 'cluster' not found yet (run Night 11 KMeans).\")\n",
        "\n",
        "if df_edges is not None:\n",
        "    print(\"\\nEdges rows:\", len(df_edges))\n",
        "    if df_videos is not None and {\"source_video_id\",\"video_id\"}.issubset(df_edges.columns):\n",
        "        vids = set(df_videos[\"video_id\"])\n",
        "        bad = df_edges[~df_edges[\"source_video_id\"].isin(vids) | ~df_edges[\"video_id\"].isin(vids)]\n",
        "        print(\"Edges referencing missing nodes:\", len(bad))\n",
        "\n",
        "# 6) Similarity artifacts\n",
        "print(\"\\n== Similarity artifacts ==\")\n",
        "try:\n",
        "    S = np.load(\"data/processed/similarity_matrix.npy\")\n",
        "    print(\"similarity_matrix.npy shape:\", S.shape)\n",
        "    ok(\"Square matrix\", S.shape[0] == S.shape[1])\n",
        "except Exception as e:\n",
        "    print(\"Could not load similarity_matrix.npy:\", e)\n",
        "\n",
        "if os.path.exists(\"data/processed/similarity_topk_edges.csv\"):\n",
        "    try:\n",
        "        df_topk = pd.read_csv(\"data/processed/similarity_topk_edges.csv\")\n",
        "        print(\"similarity_topk_edges.csv rows:\", len(df_topk))\n",
        "        ok(\"TopK required columns\", {\"src_video_id\",\"dst_video_id\",\"similarity\"}.issubset(df_topk.columns))\n",
        "    except Exception as e:\n",
        "        print(\"Could not read similarity_topk_edges.csv:\", e)\n",
        "\n",
        "# 7) Figures\n",
        "print(\"\\n== Figures ==\")\n",
        "ok(\"figures/graph_overview.png\", os.path.exists(\"figures/graph_overview.png\"))\n",
        "ok(\"figures/graph_lcc.png\", os.path.exists(\"figures/graph_lcc.png\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05c9ee20",
      "metadata": {
        "id": "05c9ee20"
      },
      "outputs": [],
      "source": [
        "# NIGHT 10–11 CHECKLIST (revised to use videos_text.csv)\n",
        "import os, json, numpy as np, pandas as pd\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/yt_mini_project\"\n",
        "\n",
        "def exists(p):\n",
        "    return os.path.exists(p)\n",
        "\n",
        "def ok(name, cond):\n",
        "    print(f\"[{'OK' if cond else 'MISS'}] {name}\")\n",
        "\n",
        "# 1) Local files expected\n",
        "local_expected = [\n",
        "    \"data/clean/edges_clean.csv\",\n",
        "    \"data/processed/similarity_matrix.npy\",\n",
        "    \"data/processed/similarity_topk_edges.csv\",\n",
        "    \"data/processed/tfidf_vectorizer.joblib\"  # if you saved via Option B\n",
        "]\n",
        "print(\"== Local files ==\")\n",
        "for p in local_expected:\n",
        "    ok(p, exists(p))\n",
        "\n",
        "# 2) Drive files expected\n",
        "drive_expected = [\n",
        "    f\"{DRIVE_BASE}/data/clean/edges_clean.csv\",\n",
        "    f\"{DRIVE_BASE}/data/processed/similarity_matrix.npy\",\n",
        "    f\"{DRIVE_BASE}/data/processed/similarity_topk_edges.csv\",\n",
        "    f\"{DRIVE_BASE}/data/processed/tfidf_vectorizer.joblib\",\n",
        "    f\"{DRIVE_BASE}/figures/graph_overview.png\",\n",
        "    f\"{DRIVE_BASE}/figures/graph_lcc.png\",\n",
        "    f\"{DRIVE_BASE}/videos_text.csv\"\n",
        "]\n",
        "print(\"\\n== Drive files ==\")\n",
        "for p in drive_expected:\n",
        "    ok(p, exists(p))\n",
        "\n",
        "# 3) Load videos_text (with text column)\n",
        "print(\"\\n== DataFrame sanity (videos_text.csv) ==\")\n",
        "try:\n",
        "    df_videos = pd.read_csv(f\"{DRIVE_BASE}/videos_text.csv\")\n",
        "    ok(\"Loaded videos_text.csv\", True)\n",
        "    ok(\"Column 'text' present\", \"text\" in df_videos.columns)\n",
        "    ok(\"Column 'seed_query' present\", \"seed_query\" in df_videos.columns)\n",
        "except Exception as e:\n",
        "    ok(\"Loaded videos_text.csv\", False)\n",
        "    print(\"Error:\", e)\n",
        "    df_videos = None\n",
        "\n",
        "# 4) Load edges_clean\n",
        "print(\"\\n== Edges check ==\")\n",
        "try:\n",
        "    df_edges = pd.read_csv(\"data/clean/edges_clean.csv\")\n",
        "    ok(\"Loaded data/clean/edges_clean.csv\", True)\n",
        "    ok(\"Edges columns present\", {\"source_video_id\",\"video_id\"}.issubset(df_edges.columns))\n",
        "except Exception as e:\n",
        "    ok(\"Loaded data/clean/edges_clean.csv\", False)\n",
        "    print(\"Error:\", e)\n",
        "    df_edges = None\n",
        "\n",
        "# 5) Shapes + clusters\n",
        "if df_videos is not None:\n",
        "    print(\"\\nVideos rows:\", len(df_videos))\n",
        "    if \"cluster\" in df_videos.columns:\n",
        "        print(\"Cluster labels found. Counts:\")\n",
        "        print(df_videos[\"cluster\"].value_counts().sort_index())\n",
        "    else:\n",
        "        print(\"Column 'cluster' not found yet (run Night 11 KMeans).\")\n",
        "\n",
        "if df_edges is not None:\n",
        "    print(\"\\nEdges rows:\", len(df_edges))\n",
        "    if df_videos is not None and {\"source_video_id\",\"video_id\"}.issubset(df_edges.columns):\n",
        "        vids = set(df_videos[\"video_id\"])\n",
        "        bad = df_edges[~df_edges[\"source_video_id\"].isin(vids) | ~df_edges[\"video_id\"].isin(vids)]\n",
        "        print(\"Edges referencing missing nodes:\", len(bad))\n",
        "\n",
        "# 6) Similarity artifacts\n",
        "print(\"\\n== Similarity artifacts ==\")\n",
        "try:\n",
        "    S = np.load(\"data/processed/similarity_matrix.npy\")\n",
        "    print(\"similarity_matrix.npy shape:\", S.shape)\n",
        "    ok(\"Square matrix\", S.shape[0] == S.shape[1])\n",
        "except Exception as e:\n",
        "    print(\"Could not load similarity_matrix.npy:\", e)\n",
        "\n",
        "if os.path.exists(\"data/processed/similarity_topk_edges.csv\"):\n",
        "    try:\n",
        "        df_topk = pd.read_csv(\"data/processed/similarity_topk_edges.csv\")\n",
        "        print(\"similarity_topk_edges.csv rows:\", len(df_topk))\n",
        "        ok(\"TopK required columns\", {\"src_video_id\",\"dst_video_id\",\"similarity\"}.issubset(df_topk.columns))\n",
        "    except Exception as e:\n",
        "        print(\"Could not read similarity_topk_edges.csv:\", e)\n",
        "\n",
        "# 7) Figures\n",
        "print(\"\\n== Figures ==\")\n",
        "ok(\"figures/graph_overview.png\", os.path.exists(\"figures/graph_overview.png\"))\n",
        "ok(\"figures/graph_lcc.png\", os.path.exists(\"figures/graph_lcc.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2cd4ab",
      "metadata": {
        "id": "2f2cd4ab"
      },
      "outputs": [],
      "source": [
        "# Night 12 — KMeans topics + entropy by seed\n",
        "import os, numpy as np, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 1) Load processed videos with 'text'\n",
        "DF_PATH = \"/content/drive/MyDrive/videos_text.csv\"   # ajuste se necessário\n",
        "df_videos = pd.read_csv(DF_PATH)\n",
        "assert \"text\" in df_videos.columns and \"seed_query\" in df_videos.columns\n",
        "\n",
        "# 2) TF-IDF (reuse if already in memory)\n",
        "try:\n",
        "    tfidf_matrix\n",
        "    _ = tfidf_matrix.shape\n",
        "except NameError:\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    tfidf_matrix = vectorizer.fit_transform(df_videos[\"text\"])\n",
        "\n",
        "print(\"TF-IDF:\", tfidf_matrix.shape)\n",
        "\n",
        "# 3) KMeans clustering\n",
        "N_CLUSTERS = 3  # você pode testar 3–6 depois\n",
        "kmeans = KMeans(n_clusters=N_CLUSTERS, n_init=10, random_state=42)\n",
        "labels = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "df_videos[\"cluster\"] = labels\n",
        "print(\"Cluster counts:\\n\", df_videos[\"cluster\"].value_counts().sort_index(), \"\\n\")\n",
        "\n",
        "# 4) Entropy by seed (topic distribution per seed)\n",
        "def entropy(counts):\n",
        "    p = counts / counts.sum()\n",
        "    p = p[p > 0]\n",
        "    return float(-(p * np.log2(p)).sum())\n",
        "\n",
        "# counts table: rows=seed, cols=cluster\n",
        "seed_cluster_counts = (\n",
        "    df_videos.groupby([\"seed_query\",\"cluster\"])\n",
        "             .size()\n",
        "             .unstack(fill_value=0)\n",
        "             .sort_index()\n",
        ")\n",
        "\n",
        "entropy_by_seed = seed_cluster_counts.apply(entropy, axis=1).rename(\"entropy\").to_frame()\n",
        "entropy_by_seed[\"n_videos\"] = seed_cluster_counts.sum(axis=1)\n",
        "\n",
        "print(\"Entropy by seed:\\n\", entropy_by_seed, \"\\n\")\n",
        "\n",
        "# 5) Save outputs\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "df_videos.to_csv(\"data/processed/videos_with_clusters.csv\", index=False)\n",
        "seed_cluster_counts.to_csv(\"data/processed/seed_cluster_counts.csv\")\n",
        "entropy_by_seed.to_csv(\"data/processed/seed_cluster_entropy.csv\")\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" - data/processed/videos_with_clusters.csv\")\n",
        "print(\" - data/processed/seed_cluster_counts.csv\")\n",
        "print(\" - data/processed/seed_cluster_entropy.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7baeae37",
      "metadata": {
        "id": "7baeae37"
      },
      "outputs": [],
      "source": [
        "# Show 10 examples per cluster (title + channel) and save samples to CSV\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "assert \"cluster\" in df_videos.columns, \"Run Night 12 first to create 'cluster' labels.\"\n",
        "\n",
        "examples = []\n",
        "for c in sorted(df_videos[\"cluster\"].unique()):\n",
        "    sub = df_videos[df_videos[\"cluster\"] == c].copy()\n",
        "    print(f\"\\n=== Cluster {c} — {len(sub)} videos ===\")\n",
        "    # show up to 10 examples in the output\n",
        "    display(sub[[\"title\", \"channel_title\"]].head(10))\n",
        "    # collect 20 random examples for a CSV (adjust n if needed)\n",
        "    examples.append(sub.sample(min(20, len(sub)), random_state=42))\n",
        "\n",
        "# Save a samples file for later review\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "df_examples = pd.concat(examples, ignore_index=True)\n",
        "df_examples.to_csv(\"data/processed/cluster_examples.csv\", index=False)\n",
        "print(\"\\nSaved: data/processed/cluster_examples.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b8a612",
      "metadata": {
        "id": "81b8a612"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) Garantias e carregamentos\n",
        "# df_videos precisa ter 'video_id', 'seed_query', 'cluster'\n",
        "assert {\"video_id\",\"seed_query\",\"cluster\"}.issubset(df_videos.columns)\n",
        "\n",
        "# Similaridade: usa a que já existe; se não, calcula agora a partir do TF-IDF\n",
        "try:\n",
        "    S  # similarity matrix already in memory\n",
        "    _ = S.shape\n",
        "except NameError:\n",
        "    try:\n",
        "        # tenta carregar do disco\n",
        "        S = np.load(\"data/processed/similarity_matrix.npy\")\n",
        "    except Exception:\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        S = cosine_similarity(tfidf_matrix)\n",
        "# zera diagonal para não selecionar o próprio vídeo\n",
        "np.fill_diagonal(S, 0.0)\n",
        "\n",
        "# Índices auxiliares\n",
        "vids = df_videos[\"video_id\"].tolist()\n",
        "id2idx = {v:i for i,v in enumerate(vids)}\n",
        "\n",
        "# 1) Parâmetros da caminhada\n",
        "SEED_START_PER_SEED = 5   # quantos vídeos iniciais por seed (passo 0)\n",
        "TOPK_PER_NODE       = 5   # quantos vizinhos cada nó traz por passo\n",
        "N_STEPS             = 3   # nº de passos (0=seeds, 1, 2, 3...)\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# 2) Selecionar seeds (passo 0) — usa amostra estável por seed\n",
        "seed_names = sorted(df_videos[\"seed_query\"].unique().tolist())\n",
        "seed_start_indices = {}\n",
        "for s in seed_names:\n",
        "    idxs = df_videos.index[df_videos[\"seed_query\"]==s].tolist()\n",
        "    pick = rng.choice(idxs, size=min(SEED_START_PER_SEED, len(idxs)), replace=False)\n",
        "    seed_start_indices[s] = set(int(i) for i in pick)\n",
        "\n",
        "# 3) Caminhada por passos (expansão cumulativa por similaridade)\n",
        "walk_records = []  # (seed, step, video_id)\n",
        "frontier = {s: set(seed_start_indices[s]) for s in seed_names}\n",
        "visited  = {s: set(seed_start_indices[s]) for s in seed_names}\n",
        "\n",
        "# passo 0 (seeds)\n",
        "for s in seed_names:\n",
        "    for i in sorted(visited[s]):\n",
        "        walk_records.append((s, 0, vids[i]))\n",
        "\n",
        "# passos 1..N\n",
        "for step in range(1, N_STEPS+1):\n",
        "    new_frontier = {s: set() for s in seed_names}\n",
        "    for s in seed_names:\n",
        "        for i in frontier[s]:\n",
        "            # pega TOPK vizinhos mais similares do nó i\n",
        "            row = S[i]\n",
        "            if TOPK_PER_NODE >= len(row):\n",
        "                cand_idx = np.argsort(-row)[:TOPK_PER_NODE]\n",
        "            else:\n",
        "                # escolha eficiente: partição + ordenação local\n",
        "                cand_idx = np.argpartition(-row, TOPK_PER_NODE)[:TOPK_PER_NODE]\n",
        "                cand_idx = cand_idx[np.argsort(-row[cand_idx])]\n",
        "            # adiciona não-visitados\n",
        "            for j in cand_idx:\n",
        "                if j not in visited[s]:\n",
        "                    visited[s].add(int(j))\n",
        "                    new_frontier[s].add(int(j))\n",
        "                    walk_records.append((s, step, vids[j]))\n",
        "    frontier = new_frontier\n",
        "\n",
        "# DataFrame com (seed, step, video_id)\n",
        "df_walk = pd.DataFrame(walk_records, columns=[\"seed_query\",\"step\",\"video_id\"])\n",
        "\n",
        "# 4) Entropia cumulativa por seed e passo (usando 'cluster' de df_videos)\n",
        "# junta cluster\n",
        "df_walk = df_walk.merge(df_videos[[\"video_id\",\"cluster\"]], on=\"video_id\", how=\"left\")\n",
        "\n",
        "def entropy_from_counts(counts):\n",
        "    p = counts / counts.sum()\n",
        "    p = p[p > 0]\n",
        "    return float(-(p * np.log2(p)).sum())\n",
        "\n",
        "# calcula cumulativo: para cada seed, do passo 0..t\n",
        "rows = []\n",
        "for s in seed_names:\n",
        "    df_s = df_walk[df_walk[\"seed_query\"]==s]\n",
        "    for t in range(0, N_STEPS+1):\n",
        "        df_cum = df_s[df_s[\"step\"]<=t]\n",
        "        counts = df_cum[\"cluster\"].value_counts().sort_index()\n",
        "        ent = entropy_from_counts(counts)\n",
        "        rows.append({\"seed_query\": s, \"step\": t, \"entropy\": ent, \"n_videos\": int(len(df_cum))})\n",
        "\n",
        "df_ent_step = pd.DataFrame(rows)\n",
        "\n",
        "# 5) Plot: Entropy vs Step (linha por seed)\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "plt.figure(figsize=(7,4))\n",
        "for s in seed_names:\n",
        "    sub = df_ent_step[df_ent_step[\"seed_query\"]==s].sort_values(\"step\")\n",
        "    plt.plot(sub[\"step\"], sub[\"entropy\"], marker=\"o\", label=s)\n",
        "plt.xlabel(\"Step (cumulative)\")\n",
        "plt.ylabel(\"Entropy of clusters\")\n",
        "plt.title(\"Entropy vs Step (cumulative by seed)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/entropy_vs_step.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# 6) Salvar artefatos\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "df_walk.to_csv(\"data/processed/walk_steps.csv\", index=False)\n",
        "df_ent_step.to_csv(\"data/processed/entropy_vs_step.csv\", index=False)\n",
        "print(\"Saved:\")\n",
        "print(\" - figures/entropy_vs_step.png\")\n",
        "print(\" - data/processed/walk_steps.csv\")\n",
        "print(\" - data/processed/entropy_vs_step.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd47a745",
      "metadata": {
        "id": "dd47a745"
      },
      "outputs": [],
      "source": [
        "# Night 14.2 — Jaccard heatmap between seeds (real data)\n",
        "import os, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Load videos with seed + video_id (prefer processed file with clusters)\n",
        "CANDIDATES = [\n",
        "    \"data/processed/videos_with_clusters.csv\",\n",
        "    \"/content/drive/MyDrive/yt_mini_project/data/processed/videos_with_clusters.csv\",\n",
        "    \"/content/drive/MyDrive/yt_mini_project/videos_text.csv\",\n",
        "]\n",
        "found = None\n",
        "for p in CANDIDATES:\n",
        "    if os.path.exists(p):\n",
        "        found = p\n",
        "        break\n",
        "assert found, \"Could not find a videos file. Make sure one of the candidates exists.\"\n",
        "\n",
        "df_videos = pd.read_csv(found)\n",
        "assert {\"seed_query\",\"video_id\"}.issubset(df_videos.columns)\n",
        "\n",
        "# 2) Build sets of video_ids per seed\n",
        "seeds = sorted(df_videos[\"seed_query\"].unique().tolist())\n",
        "sets_by_seed = {s: set(df_videos.loc[df_videos[\"seed_query\"]==s, \"video_id\"]) for s in seeds}\n",
        "\n",
        "# 3) Compute Jaccard matrix\n",
        "J = np.zeros((len(seeds), len(seeds)), dtype=float)\n",
        "for i,a in enumerate(seeds):\n",
        "    for j,b in enumerate(seeds):\n",
        "        inter = len(sets_by_seed[a] & sets_by_seed[b])\n",
        "        union = len(sets_by_seed[a] | sets_by_seed[b])\n",
        "        J[i,j] = inter/union if union else 0.0\n",
        "\n",
        "df_j = pd.DataFrame(J, index=seeds, columns=seeds)\n",
        "\n",
        "# 4) Plot heatmap (with value annotations)\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "plt.figure(figsize=(5.5, 4.5))\n",
        "im = plt.imshow(J, vmin=0, vmax=1, aspect=\"auto\")\n",
        "plt.xticks(range(len(seeds)), seeds, rotation=45, ha=\"right\")\n",
        "plt.yticks(range(len(seeds)), seeds)\n",
        "plt.colorbar(im, label=\"Jaccard\")\n",
        "\n",
        "for i in range(len(seeds)):\n",
        "    for j in range(len(seeds)):\n",
        "        plt.text(j, i, f\"{J[i,j]:.2f}\", ha=\"center\", va=\"center\", fontsize=9)\n",
        "\n",
        "plt.title(\"Jaccard similarity between seeds (videos)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/jaccard_seeds_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# 5) Save CSV\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "df_j.to_csv(\"data/processed/jaccard_seeds.csv\")\n",
        "print(\"Saved: figures/jaccard_seeds_heatmap.png, data/processed/jaccard_seeds.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c617294",
      "metadata": {
        "id": "0c617294"
      },
      "outputs": [],
      "source": [
        "# Top terms per cluster from KMeans centroids\n",
        "import numpy as np\n",
        "\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "def top_terms(cluster_id, topn=12):\n",
        "    centroid = kmeans.cluster_centers_[cluster_id]\n",
        "    idx = np.argsort(-centroid)[:topn]\n",
        "    return feature_names[idx].tolist()\n",
        "\n",
        "for c in sorted(df_videos[\"cluster\"].unique()):\n",
        "    print(f\"Cluster {c} top terms:\", top_terms(c, topn=12))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4a929e",
      "metadata": {
        "id": "dc4a929e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# carregar a matriz Jaccard já salva\n",
        "df_j = pd.read_csv(\"data/processed/jaccard_seeds.csv\", index_col=0)\n",
        "\n",
        "# diversidade = 1 - média da similaridade com outras seeds\n",
        "diversity = {}\n",
        "for seed in df_j.index:\n",
        "    others = [v for s,v in df_j.loc[seed].items() if s != seed]\n",
        "    diversity[seed] = 1 - np.mean(others)\n",
        "\n",
        "df_div = pd.DataFrame.from_dict(diversity, orient=\"index\", columns=[\"diversity\"]).sort_values(\"diversity\", ascending=False)\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(6,4))\n",
        "df_div[\"diversity\"].plot(kind=\"bar\", color=\"skyblue\")\n",
        "plt.ylabel(\"Average Diversity (1 - mean Jaccard)\")\n",
        "plt.title(\"Average Diversity per Seed\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/diversity_per_seed.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# save CSV\n",
        "df_div.to_csv(\"data/processed/diversity_per_seed.csv\")\n",
        "print(\"Saved: figures/diversity_per_seed.png, data/processed/diversity_per_seed.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcae5491",
      "metadata": {
        "id": "fcae5491"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_diversity = pd.read_csv(\"data/processed/diversity_per_seed.csv\")\n",
        "print(df_diversity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e26a472",
      "metadata": {
        "id": "8e26a472"
      },
      "outputs": [],
      "source": [
        "# Recompute diversity per seed using normalized Shannon entropy\n",
        "import os, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load videos with cluster labels (from Night 12)\n",
        "CANDIDATES = [\n",
        "    \"data/processed/videos_with_clusters.csv\",\n",
        "    \"/content/drive/MyDrive/yt_mini_project/data/processed/videos_with_clusters.csv\",\n",
        "]\n",
        "found = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
        "assert found, \"videos_with_clusters.csv not found. Run Night 12 first.\"\n",
        "df = pd.read_csv(found)\n",
        "\n",
        "assert {\"seed_query\",\"cluster\"}.issubset(df.columns)\n",
        "\n",
        "# How many clusters exist?\n",
        "K = int(df[\"cluster\"].nunique())\n",
        "assert K >= 2, f\"Need at least 2 clusters; found K={K}\"\n",
        "\n",
        "def entropy(counts):\n",
        "    p = counts / counts.sum()\n",
        "    p = p[p > 0]\n",
        "    return float(-(p * np.log2(p)).sum())\n",
        "\n",
        "# counts table: rows=seed, cols=cluster\n",
        "counts = df.groupby([\"seed_query\",\"cluster\"]).size().unstack(fill_value=0).sort_index()\n",
        "\n",
        "# Shannon entropy (0..log2 K) and normalized diversity = entropy/log2(K)\n",
        "ent = counts.apply(entropy, axis=1)\n",
        "diversity_norm = ent / np.log2(K)\n",
        "\n",
        "df_div = pd.DataFrame({\n",
        "    \"seed_query\": counts.index,\n",
        "    \"entropy\": ent.values,\n",
        "    \"diversity_norm\": diversity_norm.values,\n",
        "    \"n_videos\": counts.sum(axis=1).values\n",
        "}).sort_values(\"diversity_norm\", ascending=False)\n",
        "\n",
        "# Save table\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "out_csv = \"data/processed/diversity_per_seed.csv\"\n",
        "df_div.to_csv(out_csv, index=False)\n",
        "\n",
        "print(\"Recomputed diversity:\")\n",
        "print(df_div, \"\\n\")\n",
        "print(\"Saved:\", out_csv)\n",
        "\n",
        "# Plot bars\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(df_div[\"seed_query\"], df_div[\"diversity_norm\"])\n",
        "plt.ylabel(\"Normalized diversity (entropy / log2 K)\")\n",
        "plt.title(\"Diversity by seed\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "out_fig = \"figures/diversity_per_seed.png\"\n",
        "plt.savefig(out_fig, dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(\"Saved:\", out_fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf74d01",
      "metadata": {
        "id": "4bf74d01"
      },
      "outputs": [],
      "source": [
        "import os, pandas as pd, numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# 1) carregar dados reais\n",
        "CANDIDATES = [\n",
        "    \"data/processed/videos_with_clusters.csv\",\n",
        "    \"/content/drive/MyDrive/yt_mini_project/data/processed/videos_with_clusters.csv\",\n",
        "    \"/content/drive/MyDrive/yt_mini_project/videos_text.csv\",\n",
        "]\n",
        "found = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
        "assert found, \"Nenhum arquivo de vídeos encontrado.\"\n",
        "df = pd.read_csv(found)\n",
        "assert {\"seed_query\",\"video_id\",\"title\"}.issubset(df.columns)\n",
        "\n",
        "print(\"Loaded:\", found, \"| rows:\", len(df))\n",
        "print(\"Seeds:\", sorted(df['seed_query'].unique().tolist()))\n",
        "\n",
        "# 2) mapa seed -> conjuntos\n",
        "sets_vid = {s: set(df.loc[df.seed_query==s, \"video_id\"]) for s in df.seed_query.unique()}\n",
        "sets_ch  = {s: set(df.loc[df.seed_query==s, \"channel_title\"].dropna()) for s in df.seed_query.unique()}\n",
        "\n",
        "# 3) interseção de video_ids entre seeds\n",
        "print(\"\\n== Overlap de VIDEO_IDs entre seeds ==\")\n",
        "for a,b in combinations(sorted(sets_vid.keys()), 2):\n",
        "    inter = sets_vid[a] & sets_vid[b]\n",
        "    print(f\"{a} ∩ {b}: {len(inter)}\")\n",
        "    if len(inter) > 0:\n",
        "        ex = list(inter)[:5]\n",
        "        print(\"  exemplos:\")\n",
        "        disp = df[df.video_id.isin(ex)][[\"video_id\",\"title\",\"seed_query\"]].drop_duplicates()\n",
        "        display(disp)\n",
        "\n",
        "# 4) checar se algum video_id aparece com MAIS de uma seed\n",
        "dups = (df.duplicated(\"video_id\", keep=False))\n",
        "multi_seed = (df[dups].groupby(\"video_id\")[\"seed_query\"].nunique()>1)\n",
        "multi_seed_ids = multi_seed[multi_seed].index.tolist()\n",
        "print(f\"\\nVideo_ids presentes em MULTIPLAS seeds: {len(multi_seed_ids)}\")\n",
        "if multi_seed_ids:\n",
        "    display(df[df.video_id.isin(multi_seed_ids)][[\"video_id\",\"title\",\"seed_query\"]].drop_duplicates().head(20))\n",
        "\n",
        "# 5) (sanity) interseção por CANAL\n",
        "print(\"\\n== Overlap de CANAIS entre seeds ==\")\n",
        "for a,b in combinations(sorted(sets_ch.keys()), 2):\n",
        "    inter = sets_ch[a] & sets_ch[b]\n",
        "    print(f\"{a} ∩ {b}: {len(inter)}\")\n",
        "    if len(inter) > 0:\n",
        "        print(\"  exemplos:\", list(sorted(inter))[:10])\n",
        "\n",
        "# 6) Jaccard por video_id (confirmando seu heatmap)\n",
        "seeds = sorted(sets_vid.keys())\n",
        "J = np.zeros((len(seeds), len(seeds)))\n",
        "for i,a in enumerate(seeds):\n",
        "    for j,b in enumerate(seeds):\n",
        "        inter = len(sets_vid[a] & sets_vid[b])\n",
        "        union = len(sets_vid[a] | sets_vid[b])\n",
        "        J[i,j] = inter/union if union else 0.0\n",
        "print(\"\\nMatriz Jaccard (videos):\")\n",
        "print(pd.DataFrame(J, index=seeds, columns=seeds).round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50223f5",
      "metadata": {
        "id": "a50223f5"
      },
      "source": [
        "## Data collection\n",
        "Queries YouTube Data API v3 using **seed queries** (e.g., *healthy cooking*, *beginner guitar*, *stretching exercises*). Collects a pool per seed with `search.list`, storing `video_id`, `title`, `description`, `channel_title`.\n",
        "Note: we no longer rely on `relatedToVideoId` (YouTube has limited that path); instead we approximate relatedness later via content similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a5e454",
      "metadata": {
        "id": "05a5e454"
      },
      "outputs": [],
      "source": [
        "def search_paged(query, max_items=150):\n",
        "    \"\"\"\n",
        "    Fetch up to `max_items` videos for a query using pageToken pagination.\n",
        "    Returns a DataFrame with: seed_query, video_id, title, description, channel_title.\n",
        "    \"\"\"\n",
        "    rows, token = [], None\n",
        "    while len(rows) < max_items:\n",
        "        params = {\n",
        "            \"part\": \"snippet\", \"q\": query, \"type\": \"video\",\n",
        "            \"maxResults\": 50, \"key\": API_KEY\n",
        "        }\n",
        "        if token:\n",
        "            params[\"pageToken\"] = token\n",
        "        r = requests.get(SEARCH_URL, params=params)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "\n",
        "        for it in data.get(\"items\", []):\n",
        "            rows.append({\n",
        "                \"seed_query\": query,\n",
        "                \"video_id\": it[\"id\"][\"videoId\"],\n",
        "                \"title\": it[\"snippet\"].get(\"title\",\"\"),\n",
        "                \"description\": it[\"snippet\"].get(\"description\",\"\"),\n",
        "                \"channel_title\": it[\"snippet\"].get(\"channelTitle\",\"\"),\n",
        "            })\n",
        "            if len(rows) >= max_items:\n",
        "                break\n",
        "\n",
        "        token = data.get(\"nextPageToken\")\n",
        "        if not token:\n",
        "            break\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def build_edges_tfidf(df_pool, seeds_df, k=5):\n",
        "    \"\"\"\n",
        "    For each seed (title+description), returns top-k most similar videos from the pool.\n",
        "    Output columns: seed_query, source_video_id, video_id, title, channel_title, similarity.\n",
        "    \"\"\"\n",
        "    text = (df_pool[\"title\"].fillna(\"\") + \" \" + df_pool[\"description\"].fillna(\"\")).values\n",
        "    tfidf = TfidfVectorizer(lowercase=True, stop_words=\"english\", max_features=20000)\n",
        "    X = tfidf.fit_transform(text)\n",
        "    idx_by_id = {vid: i for i, vid in enumerate(df_pool[\"video_id\"].tolist())}\n",
        "\n",
        "    edges = []\n",
        "    for _, seed in seeds_df.iterrows():\n",
        "        seed_text = (str(seed.get(\"title\",\"\")) + \" \" + str(seed.get(\"description\",\"\"))).strip()\n",
        "        x_seed = tfidf.transform([seed_text])\n",
        "        sims = cosine_similarity(x_seed, X)[0]\n",
        "        # exclude the seed itself if it’s in the pool\n",
        "        if seed[\"video_id\"] in idx_by_id:\n",
        "            sims[idx_by_id[seed[\"video_id\"]]] = -1.0\n",
        "        top_idx = np.argsort(-sims)[:k]\n",
        "        for i in top_idx:\n",
        "            row = df_pool.iloc[i]\n",
        "            edges.append({\n",
        "                \"seed_query\": seed[\"seed_query\"],\n",
        "                \"source_video_id\": seed[\"video_id\"],\n",
        "                \"video_id\": row[\"video_id\"],\n",
        "                \"title\": row[\"title\"],\n",
        "                \"channel_title\": row[\"channel_title\"],\n",
        "                \"similarity\": float(sims[i]),\n",
        "            })\n",
        "    return pd.DataFrame(edges)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d7fdfb1",
      "metadata": {
        "id": "0d7fdfb1"
      },
      "source": [
        "## Preprocessing\n",
        "Lower-case, de-HTML (`&amp;`→`and`), strip punctuation, and concatenate `title + description` into a `text` column. Remove duplicates by `video_id` and save clean tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be321fc4",
      "metadata": {
        "id": "be321fc4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Text field: title + description\n",
        "pool_text = (df_pool[\"title\"].fillna(\"\") + \" \" + df_pool[\"description\"].fillna(\"\")).values\n",
        "\n",
        "# Fit TF-IDF on the pool\n",
        "tfidf = TfidfVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words=\"english\",      # simple English stopwords\n",
        "    max_features=20000\n",
        ")\n",
        "X_pool = tfidf.fit_transform(pool_text)\n",
        "\n",
        "# Map video_id -> row index\n",
        "idx_by_id = {vid: i for i, vid in enumerate(df_pool[\"video_id\"].tolist())}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00aae513",
      "metadata": {
        "id": "00aae513"
      },
      "outputs": [],
      "source": [
        "# Nodes: union of seeds + pool (you can enrich later with stats)\n",
        "df_nodes = df_pool.drop_duplicates(subset=[\"video_id\"]).copy()\n",
        "df_nodes.to_csv(\"videos_pool.csv\", index=False)\n",
        "df_edges_step1.to_csv(\"edges_step1_tfidf.csv\", index=False)\n",
        "\n",
        "print(\"Saved: videos_pool.csv and edges_step1_tfidf.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6149491a",
      "metadata": {
        "id": "6149491a"
      },
      "outputs": [],
      "source": [
        "SEEDS       = [\"healthy cooking\", \"beginner guitar\", \"stretching exercises\"]\n",
        "POOL_SIZE   = 150   # videos per seed (≈3 pages)\n",
        "SEED_COUNT  = 5     # seed videos per query\n",
        "TOPK        = 5     # neighbors per seed video\n",
        "\n",
        "all_nodes, all_edges = [], []\n",
        "\n",
        "for q in SEEDS:\n",
        "    df_pool = search_paged(q, max_items=POOL_SIZE)\n",
        "    df_seed_local = df_pool.head(SEED_COUNT).copy()\n",
        "    df_edges = build_edges_tfidf(df_pool, df_seed_local, k=TOPK)\n",
        "\n",
        "    all_nodes.append(df_pool)\n",
        "    all_edges.append(df_edges)\n",
        "    print(f\"[{q}] pool={len(df_pool)} edges={len(df_edges)}\")\n",
        "\n",
        "# Concatenate & dedupe\n",
        "df_nodes_raw = pd.concat(all_nodes, ignore_index=True).drop_duplicates(subset=[\"video_id\"])\n",
        "df_edges_raw = pd.concat(all_edges, ignore_index=True).drop_duplicates()\n",
        "\n",
        "# Save to data/raw/\n",
        "nodes_path = \"data/raw/videos_raw.csv\"\n",
        "edges_path = \"data/raw/edges_raw.csv\"\n",
        "df_nodes_raw.to_csv(nodes_path, index=False)\n",
        "df_edges_raw.to_csv(edges_path, index=False)\n",
        "\n",
        "print(f\"\\nSaved:\\n  {nodes_path} ({len(df_nodes_raw)} rows)\\n  {edges_path} ({len(df_edges_raw)} rows)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01ae6024",
      "metadata": {
        "id": "01ae6024"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", str(text).lower())  # remove tudo que não é letra\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]  # remove stopwords e palavras curtas\n",
        "    return tokens\n",
        "\n",
        "df_videos[\"tokens\"] = df_videos[\"text\"].apply(tokenize)\n",
        "df_videos[[\"text\", \"tokens\"]].head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6263b9",
      "metadata": {
        "id": "7b6263b9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Clean text (split, remove stopwords, punctuation)\n",
        "def tokenize(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # keep only letters + space\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# 2. Tokenize all rows\n",
        "all_tokens = []\n",
        "for t in df_videos[\"text\"]:\n",
        "    all_tokens.extend(tokenize(t))\n",
        "\n",
        "print(\"Total tokens:\", len(all_tokens))\n",
        "\n",
        "# 3. Count frequencies\n",
        "counter = Counter(all_tokens)\n",
        "most_common = counter.most_common(20)\n",
        "\n",
        "# 4. Plot top 20 words\n",
        "words, counts = zip(*most_common)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(words, counts)\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.title(\"Top 20 most frequent words\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da8d952e",
      "metadata": {
        "id": "da8d952e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def tokenize(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "    return tokens\n",
        "\n",
        "# Tokenize all rows and count\n",
        "all_tokens = [tok for txt in df_videos[\"text\"] for tok in tokenize(txt)]\n",
        "counter = Counter(all_tokens)\n",
        "most_common = counter.most_common(20)\n",
        "\n",
        "# Plot again\n",
        "words, counts = zip(*most_common)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(words, counts, color=\"skyblue\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.title(\"Top 20 words (stopwords removed)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8245ef8",
      "metadata": {
        "id": "d8245ef8"
      },
      "source": [
        "## TF‑IDF & Similarity\n",
        "Vectorize `text` with `TfidfVectorizer(min_df=2)` and compute cosine similarity. Persist the TF‑IDF model and (optionally) a similarity matrix or top‑K neighbor pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9072602a",
      "metadata": {
        "id": "9072602a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f19164d",
      "metadata": {
        "id": "8f19164d"
      },
      "outputs": [],
      "source": [
        "# Transform texts into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # limita para evitar explosão\n",
        "tfidf_matrix = vectorizer.fit_transform(df_videos[\"text\"])\n",
        "\n",
        "print(\"TF-IDF shape:\", tfidf_matrix.shape)  # (n_samples, n_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf1b018",
      "metadata": {
        "id": "abf1b018"
      },
      "outputs": [],
      "source": [
        "# Compute cosine similarity between all videos\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "print(\"Similarity matrix shape:\", similarity_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65cfdc3d",
      "metadata": {
        "id": "65cfdc3d"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(vectorizer, \"data/processed/tfidf_vectorizer.joblib\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c41ddfb",
      "metadata": {
        "id": "8c41ddfb"
      },
      "outputs": [],
      "source": [
        "S = cosine_similarity(tfidf_matrix)\n",
        "np.save(\"data/processed/similarity_matrix.npy\", S)\n",
        "# ... e o bloco que gera similarity_topk_edges.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976b26dd",
      "metadata": {
        "id": "976b26dd"
      },
      "source": [
        "## Clustering\n",
        "Run `KMeans` to group videos into coarse topical clusters. Append `cluster` labels to the videos table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c69de0a0",
      "metadata": {
        "id": "c69de0a0"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "N_CLUSTERS = 3  # you can try 3–6 later\n",
        "kmeans = KMeans(n_clusters=N_CLUSTERS, n_init=10, random_state=42)\n",
        "labels = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "df_videos[\"cluster\"] = labels\n",
        "print(df_videos[\"cluster\"].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b457608e",
      "metadata": {
        "id": "b457608e"
      },
      "source": [
        "## Metrics\n",
        "Compute simple indicators: normalized entropy of cluster distribution per seed (diversity proxy), Jaccard overlap between seeds (by `video_id`), and an *entropy vs. step* curve to approximate how diversity evolves as the user keeps consuming content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c7124e",
      "metadata": {
        "id": "d8c7124e"
      },
      "outputs": [],
      "source": [
        "# most frequent seed per video_id\n",
        "seed_per_video = (\n",
        "    df_videos.groupby(\"video_id\")[\"seed_query\"]\n",
        "    .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else \"unknown\")\n",
        "    .to_dict()\n",
        ")\n",
        "list(seed_per_video.items())[:5]  # quick peek\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660046f1",
      "metadata": {
        "id": "660046f1"
      },
      "outputs": [],
      "source": [
        "components = sorted(nx.connected_components(G), key=len, reverse=True)\n",
        "G_main = G.subgraph(components[0]).copy()\n",
        "\n",
        "pos = nx.spring_layout(G_main, seed=42, k=0.35)\n",
        "deg_main = dict(G_main.degree())\n",
        "node_sizes = [max(50, deg_main[n]*60) for n in G_main.nodes()]\n",
        "node_colors = [seed_to_int[G_main.nodes[n][\"seed\"]] for n in G_main.nodes()]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "nx.draw_networkx_edges(G_main, pos, alpha=0.25, width=0.9)\n",
        "nx.draw_networkx_nodes(G_main, pos, node_color=node_colors, node_size=node_sizes, cmap=plt.cm.tab10, alpha=0.95)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Largest connected component\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/graph_lcc.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(\"Saved figure to: figures/graph_lcc.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c665cf57",
      "metadata": {
        "id": "c665cf57"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    df_videos.groupby([\"cluster\",\"seed_query\"])\n",
        "             .size()\n",
        "             .unstack(fill_value=0)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18befd7",
      "metadata": {
        "id": "a18befd7"
      },
      "source": [
        "## Appendix (utilities)\n",
        "Helper snippets retained for completeness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d3528e0",
      "metadata": {
        "id": "1d3528e0"
      },
      "outputs": [],
      "source": [
        "def search_videos_by_query(query, max_results=25, region_code=\"US\", outfile=None):\n",
        "    \"\"\"\n",
        "    Search videos by keyword, enrich with category name and view count,\n",
        "    and optionally save to CSV. Returns a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    # 1) Get candidate video IDs (search.list only returns lightweight info)\n",
        "    search_resp = youtube.search().list(\n",
        "        part=\"snippet\",\n",
        "        q=query,\n",
        "        type=\"video\",\n",
        "        maxResults=min(max_results, 50)  # API per-call limit\n",
        "    ).execute()\n",
        "\n",
        "    items = search_resp.get(\"items\", [])\n",
        "    video_ids = [it[\"id\"][\"videoId\"] for it in items]\n",
        "\n",
        "    if not video_ids:\n",
        "        print(\"No videos found for:\", query)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 2) Fetch full details/statistics for those IDs (videos.list)\n",
        "    videos_resp = youtube.videos().list(\n",
        "        part=\"snippet,statistics\",\n",
        "        id=\",\".join(video_ids)\n",
        "    ).execute()\n",
        "\n",
        "    cat_map = get_category_map(region_code)\n",
        "\n",
        "    rows = []\n",
        "    for v in videos_resp.get(\"items\", []):\n",
        "        vid   = v[\"id\"]\n",
        "        snip  = v[\"snippet\"]\n",
        "        stats = v.get(\"statistics\", {})\n",
        "        cat_id = snip.get(\"categoryId\", \"\")\n",
        "        rows.append({\n",
        "            \"query\": query,\n",
        "            \"video_id\": vid,\n",
        "            \"title\": snip.get(\"title\", \"\"),\n",
        "            \"channel_title\": snip.get(\"channelTitle\", \"\"),\n",
        "            \"category_id\": cat_id,\n",
        "            \"category_name\": cat_map.get(cat_id, \"\"),\n",
        "            \"view_count\": int(stats.get(\"viewCount\", 0))\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "    if outfile:\n",
        "        df.to_csv(outfile, index=False)\n",
        "        print(f\"Saved: {outfile}  ({len(df)} rows)\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1334c6d",
      "metadata": {
        "id": "a1334c6d"
      },
      "outputs": [],
      "source": [
        "def get_related_videos(video_id, max_results=10):\n",
        "    \"\"\"\n",
        "    Fetch up to `max_results` related videos for a given video_id.\n",
        "    Returns a list of dicts with (video_id, title, channel_title).\n",
        "    \"\"\"\n",
        "    resp = youtube.search().list(\n",
        "        part=\"snippet\",\n",
        "        relatedToVideoId=video_id,\n",
        "        type=\"video\",\n",
        "        maxResults=min(max_results, 50)\n",
        "    ).execute()\n",
        "\n",
        "    related = []\n",
        "    for it in resp.get(\"items\", []):\n",
        "        rid = it[\"id\"][\"videoId\"]\n",
        "        snip = it[\"snippet\"]\n",
        "        related.append({\n",
        "            \"video_id\": rid,\n",
        "            \"title\": snip.get(\"title\", \"\"),\n",
        "            \"channel_title\": snip.get(\"channelTitle\", \"\")\n",
        "        })\n",
        "    return related\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de50983",
      "metadata": {
        "id": "6de50983"
      },
      "outputs": [],
      "source": [
        "def get_related_videos(video_id, max_results=10):\n",
        "    \"\"\"\n",
        "    Fetch up to `max_results` related videos for a given video_id.\n",
        "    Returns a list of dicts with (video_id, title, channel_title).\n",
        "    \"\"\"\n",
        "    resp = youtube.search().list(\n",
        "        part=\"snippet\",\n",
        "        relatedToVideoId=video_id,  # requires type='video'\n",
        "        type=\"video\",\n",
        "        maxResults=min(max_results, 50)\n",
        "    ).execute()\n",
        "\n",
        "    related = []\n",
        "    for it in resp.get(\"items\", []):\n",
        "        rid = it[\"id\"][\"videoId\"]\n",
        "        snip = it[\"snippet\"]\n",
        "        related.append({\n",
        "            \"video_id\": rid,\n",
        "            \"title\": snip.get(\"title\", \"\"),\n",
        "            \"channel_title\": snip.get(\"channelTitle\", \"\")\n",
        "        })\n",
        "    return related\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c28574ff",
      "metadata": {
        "id": "c28574ff"
      },
      "source": [
        "---\n",
        "**Reproducibility note:** this notebook avoids hard‑coding secrets, writes deterministic artifacts to `data/` and `figures/`, and is designed to run end‑to‑end on a fresh Colab runtime."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}